---
title: "Two Years of Tweets"
author: "Mara Averick"
date: "2017-06-15"
date_updated: "`r Sys.Date()`"
output: 
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, fig.path = 'fig/', dev = 'png', dpi = 100, fig.retina = 2)
```

Updated: `r Sys.Date()`

## Motivation

In their very excellent book, [**Text Mining with R: A Tidy Approach**](http://tidytextmining.com/), [Julia Silge](http://juliasilge.com/) and [David Robinson](http://varianceexplained.org/) include a [case study](http://tidytextmining.com/twitter.html) comparing their respective twitter archives.[^1] They do so using the R package they co-authored, [`tidytext`](https://github.com/juliasilge/tidytext), which is a lot of fun to use. So, since I like playing with `tidytext`, and obviously want to be cool like Julia and Dave, I thought I'd poke around in my own tweets of yore.[^2] As I do not have a partner in crime with whom to compare my tweets, I'll also draw on an older (but still awesome in its own right) post from Julia's blog, [_Ten Thousand Tweets_](https://juliasilge.com/blog/ten-thousand-tweets/).

## Getting the data

I'm literally following the [TidyText guide](http://tidytextmining.com/twitter.html), so just mosey over there for a step-by-step break down.

```{r libraries, message=FALSE}
library(tidyverse)
library(lubridate)
library(scales)
library(hrbrthemes)
```

```{r read_data, message=FALSE}
# read in data
tweets <- read_csv("data/tweets.csv",
  col_types = cols(in_reply_to_status_id = col_character(),
                   in_reply_to_user_id = col_character(),
                   retweeted_status_id = col_character(),
                   retweeted_status_user_id = col_character(),
                   tweet_id = col_character()))

# convert datetimes using lubridate
tweets <- tweets %>%
  mutate(timestamp = ymd_hms(timestamp)) %>%
  mutate(retweeted_status_timestamp = ymd_hms(retweeted_status_timestamp)) %>%
  mutate(person = "Mara") %>%  ## add Mara as person
  arrange(desc(timestamp))  ## arrange descending by date
```

We can take a look at our shiny new data frame using `glimpse()`.

```{r glimpse}
glimpse(tweets)
```

The data go (temporally-speaking) from when I first tweeted, to when I downloaded the archive (2017-06-15). Let's find out when that first tweet went out. 

```{r min_timestamp}
first_tweet <- tweets %>%
  summarise(min(timestamp))

first_tweet
```

## Tweets over time

So, we've basically got two years and one month worth of tweets. Let's take a look at the distribution of tweets over time. Since we've got 25 months, we'll use 25 bins, so that they each cover approximately a month. I'll be using `theme_ipsum_rc()`, which is one of the themes in Bob Rudis' (a.k.a. [\@hrbrmstr](https://twitter.com/hrbrmstr))'s aptly named [`hrbrthemes`](https://hrbrmstr.github.io/hrbrthemes/) package. 

```{r tweet_hist}
ggplot(tweets, aes(x = timestamp)) +
  geom_histogram(position = "identity", bins = 24, show.legend = FALSE, color = "white", alpha = 0.9) +
  labs(title = "@dataandme tweet volume",
       caption = "Extracted from Twitter archive 2017-06-15") +  ## use labs w/ hrbrthemes
  theme_ipsum_rc()
```

From the look of things, I've been tweeting up a storm of late.

The `timestamp` variable contains quite a bit of information, so it's worth tidying things up. We can use the datetime to get the date, and put it into `yyyy-mm-dd` format, which can then be separated into: year, month, and day.[^3]


```{r tidy date}
tweets <- tweets %>%
  mutate(date = as_date(timestamp)) %>%
  separate(date, into = c("year", "month", "day"), sep = "-", remove = FALSE)
```

Let's take a look at how my monthly tweet volume has changed over the years.

```{r hist by year, message=FALSE, warning=FALSE}
ggplot(tweets, aes(x = month, fill = year)) +
  geom_histogram(position = "identity", stat = "count", show.legend = FALSE, alpha = 0.9) +
  facet_wrap(~year, ncol = 1) +
  theme_minimal() +
  theme(strip.text.x = element_text(face = "bold"),
        panel.grid.minor = element_blank())
```

What about the days of the week?

```{r tweets by day, warning=FALSE}
ggplot(data = tweets, aes(x = wday(timestamp, label = TRUE))) +
        geom_histogram(breaks = seq(0.5, 7.5, by =1), stat = "count", show.legend = FALSE, color = "white", alpha = 0.9) +
        theme(legend.position = "none") +
        labs(title = "@dataandme tweets by day of the week",
             x = "days of the week",
             caption = "Extracted from Twitter archive 017-06-15") +
        theme_ipsum_rc()
```

It _looks_ like, in the aggregate, I'm pretty darn consistent. But, let's run a chi-squared test to see if the variation in tweet distribution is _really_ in the realm of random sampling error.[^4]

```{r chisq}
chisq.test(table(wday(tweets$timestamp, label = TRUE)))
```

Well well well... Looks _can_ be deceiving.

## What's in those tweets?

Well, let's take [`tidytext`](https://github.com/juliasilge/tidytext) out for a spin and see.[^word_freq] We'll also need to load the `stringr` package because, although it is part of the [`tidyverse`](http://tidyverse.org/) collection of packages, it's not among the **core** packages that are automatically loaded when you run `library(tidyverse)`.

```{r tidy tweets}
library(tidytext)
library(stringr)

replace_reg <- "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https"
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
tidy_tweets <- tweets %>% 
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = unnest_reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))
```

By using `unnest_tokens()`, I broke all of the non-re-tweet tweets down into individual words.

```{r word frequency}
frequency <- tidy_tweets %>% 
  group_by(person) %>% 
  count(word, sort = TRUE) %>% 
  left_join(tidy_tweets %>% 
              group_by(person) %>% 
              summarise(total = n())) %>%
  mutate(freq = n/total)

frequency
```

```{r filter by date}
tidy_tweets <- tidy_tweets %>%
  filter(timestamp >= as.Date("2016-05-1"),
         timestamp < as.Date("2017-05-17"))
```

```{r words by time}
words_by_time <- tidy_tweets %>%
  filter(!str_detect(word, "^@")) %>%
  mutate(time_floor = floor_date(timestamp, unit = "1 month")) %>%
  count(time_floor, person, word) %>%
  ungroup() %>%
  group_by(person, time_floor) %>%
  mutate(time_total = sum(n)) %>%
  group_by(word) %>%
  mutate(word_total = sum(n)) %>%
  ungroup() %>%
  rename(count = n) %>%
  filter(word_total > 30)

words_by_time
```

```{r nested data}
nested_data <- words_by_time %>%
  nest(-word, -person) 

nested_data
```

```{r nested models}
library(purrr)

nested_models <- nested_data %>%
  mutate(models = map(data, ~ glm(cbind(count, time_total) ~ time_floor, ., 
                                  family = "binomial")))

nested_models
```

```{r slopes}
library(broom)

slopes <- nested_models %>%
  unnest(map(models, tidy)) %>%
  filter(term == "time_floor") %>%
  mutate(adjusted.p.value = p.adjust(p.value))
```

```{r top slopes}
top_slopes <- slopes %>% 
  filter(adjusted.p.value < 0.1)

top_slopes
```

```{r trending words, fig.width=10}
words_by_time %>%
  inner_join(top_slopes, by = c("word", "person")) %>%
  ggplot(aes(time_floor, count/time_total, color = word)) +
  geom_line(size = 1) +
  labs(title = "Trending words in @dataandme tweets", 
       x = NULL, y = "Word frequency") +
  theme_ipsum_rc()
```


----
## All this made possible by...

```{r session_info, include=FALSE}
sessionInfo()
```

```{r thankr}
library(thankr)
thankees <- shoulders()
library(DT)
datatable(thankees)
```

```{r praise}
library(praise)
praise::praise()
```


[^1]: Julia ([\@juliasilge](https://twitter.com/juliasilge)), and Dave ([\@drob](https://twitter.com/drob)) are excellent tweeters, well worth following-- though, given that you're reading this, I'm guessing the probability that you already do so is pretty high.

[^2]: See Twitter's [_Downloading your Twitter archive_](https://support.twitter.com/articles/20170160#) if you want to check out your own 140-character collection.

[^3]: I'm also setting the `separate()` argument `remove = FALSE`. This way we keep the date, and have individual year, month, and day values.

[^4]: Also, as previously mentioned, I want to be super cool like Julia, and she did this for her [Ten Thousand Tweets](https://juliasilge.com/blog/ten-thousand-tweets/) analysis. So, there.

[^word_freq]: Again, I'm still following [Tidy Text Mining with R](http://tidytextmining.com), specifically [*7.2 Word frequencies*](http://tidytextmining.com/twitter.html#word-frequencies-1) here.
